exp:
    exp_dir: /content/experiments
    device: cuda
    seed: 777
    use_wandb: True
    model_type: diffusion

data:
    dataset: food101_dataset
    input_train_dir: /content/train
    input_val_dir: /content/test
    train_batch_size: 64
    val_batch_size: 8
    workers: 8

train:
    trainer: improved_diffusion_trainer
    model: improved_diffusion
    diffusion: spaced_diffusion
    optimizer: adam
    noise_sampler: uniform_sampler
    checkpoint_path: ""
    checkpoint_save_path: /content/drive/MyDrive/checkpoints
    val_metrics: ["fid"]
    start_step: 0
    steps: 300000
    log_step: 500
    checkpoint_step: 15000
    val_step: 15000
    val_images_per_class: 20
    val_sample_images: 5
    # crutch to log correct fid and images
    validation_run: False
    validation_checkpoints: [
        "/content/drive/MyDrive/checkpoints/checkpoint_improved_diffusion_step_15000.pkl",
        "/content/drive/MyDrive/checkpoints/checkpoint_improved_diffusion_step_30000.pkl",
        "/content/drive/MyDrive/checkpoints/checkpoint_improved_diffusion_step_45000.pkl"
    ]

# this configuration uses rescaled mse as a learning objective
# if we enable learn_sigma, then we will use hybrid loss, presented
# in the paper MSE + VLB
# enabling KL will just learn variational lower bound (VLB)

model_args:
    # loss is embedded into forward diffusion process
    learn_sigma: True
    forward_process_args:
        sigma_small: False
        steps: 200
        noise_schedule: cosine
        timestep_respacing: ""
        use_kl: False
        predict_xstart: False
        rescale_timesteps: True
        rescale_learned_sigmas: True
    reverse_process_args:
        image_size: 64
        num_channels: 128
        num_res_blocks: 2
        num_heads: 4
        num_heads_upsample: -1
        attention_resolutions: "16,8"
        dropout: 0.0
        class_cond: True
        use_checkpoint: False
        use_scale_shift_norm: True
        num_classes :  101

optimizer_args:
    lr: 0.0001
